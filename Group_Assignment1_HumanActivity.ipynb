{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN-ZzSPFEQRg"
   },
   "source": [
    "# Group Assignment 1: Human Activity Detection\n",
    "In this assignment you will create you own dataset for classification. You will explore which ML algorithms are best to classify this and you will present your best solution. \n",
    "\n",
    "- Create your own dataset for custom human motions using Phyphox\n",
    "- There should be at least 3 distinct types of motions\n",
    "- The motions should be different to the ones used in the UCI dataset (Not: walking, sitting, standing, laying, stairs)\n",
    "- Follow the steps and answer the questions given in this notebook\n",
    "\n",
    "### Generating your dataset:\n",
    "\n",
    "For this assignment you will create your own dataset of motions that you collect with an Accelerometer and Gyroscope. For this you can use your phone as a sensor.\n",
    "To be able to collect your data you can best use an app called [phyphox](https://phyphox.org/), this is a free app available in app stores. This app can be configured to acces your sensordata, sample it as given frequency's. you can set it up te have experiment timeslots, and the data with a timestamp can be exported to a needed output format.\n",
    "\n",
    "![](https://phyphox.org/wp-content/uploads/2019/06/phyphox_dark-1024x274.png)\n",
    "\n",
    "When you installed the app you can setup a custum experiment by clicking on the + button. Define an experiment name, sample frequency and activate the Accelerometer and Gyroscope. Your custom experiment will be added, you can run it pressing the play button and you will see sensor motion. Pressing the tree dots (...) lets you define timed runs, remote access and exporting data.\n",
    "\n",
    "Phyphox will generate 2 files with sensor data, one for the Accelerometer and one for the Giro. Both files will have timestamps which might not match the recorded sensor data for each sensor. Please, preprocess and merge the files for using it as your dataset for training, testing and deploying your own supervised learning model.\n",
    "\n",
    "### steps\n",
    "\n",
    "With your own generated dataset the similar sequence of steps should be taken to train your model.\n",
    "\n",
    "These are the generic steps to be taken\n",
    "1. Frame the problem and look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore the data to gain insights.\n",
    "4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms\n",
    "5. Explore many different models and short-list the best ones.\n",
    "6. Fine-tune your models and combine them into a great solution.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system.\n",
    "9. Additional Questions\n",
    "\n",
    "\n",
    "---\n",
    "In the Notebook this structure is used for dividing the different steps, so make sure you do the implementation and analisis at these location in the notebook. \n",
    "\n",
    "You may add additinal code blocks, but keep the seperation of the given structure.\n",
    "\n",
    "At the end of each block summarize / comment / conclude your current step in the given textblocks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVvVxhvpne6O"
   },
   "source": [
    "```\n",
    "Project group 20:\n",
    "Lars Claassen   - 4159632\n",
    "Tonnie Bour     - 4130456\n",
    "Pjotr Maes      - 3839001\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVhu3sWzEQRv"
   },
   "source": [
    "# 1. Frame the problem and look at the big picture\n",
    "Describe the problem at hand and explain your approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDdDUEHGgRe_"
   },
   "source": [
    "```\n",
    "A model is designed to classify human activities. To achieve this, data is gathered to train multiple models for classification.\n",
    "\n",
    "We use our phones and their built-in sensors to track the following activities:\n",
    "    - Casual walking\n",
    "    - Walking while looking at the phone\n",
    "    - Drinking\n",
    "For each activity, gyroscope, acceleration, and linear acceleration data are recorded and logged. Each activity is measured separately in 10-second intervals. The recorded data is then compressed into various features, including minimum, maximum, mean, and standard deviation values, resulting in 38 features per measurement.\n",
    "\n",
    "The compressed values from each measurement are compiled into a single dataset, which is then split into training and testing sets. Different model types are trained and evaluated to determine the most suitable one, providing insight into which model performs best with our data.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkPzAWZ6d2PY"
   },
   "source": [
    "# 2. Get the data.\n",
    "\n",
    "Initialize the system, get all needed libraries, retreive the data and import it\n",
    "\n",
    "> Create your own dataset\n",
    "\n",
    "> Explain and show (with a few images) which motions you are classifing, how you generated them, what the problems where you encountered in this process! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "To gather the data we used an app called Phyphox. Within this app we created our own exercise that tracks the gyroscope, acceleration, and linear acceleration. This data is logged and saved per activity. Each activity/measurement records for about 10 seconds, this is for each exercise the same to prevend a bias from forming towards the \"longer during\" activity.\n",
    "\n",
    "For casual walking we held the phone in the palm of our hand while swinging it as you would naturally do while walking. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/W1.jpg\" alt=\"Person holding phone\" width=\"300\">\n",
    "<img src=\"Images/W2.jpg\" alt=\"Person holding phone\" width=\"300\">\n",
    "<img src=\"Images/W3.jpg\" alt=\"Person holding phone\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "To measure walking while looking at the phone a subject would walk with the phone as if he/she would be texting or looking at their phone. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/WW1.jpg\" alt=\"Person holding phone\" width=\"300\">\n",
    "<img src=\"Images/WW2.jpg\" alt=\"Person holding phone\" width=\"300\">\n",
    "<img src=\"Images/WW3.jpg\" alt=\"Person holding phone\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "To measure drinking a subject would hold the phone in the hand together with their beverage (of their choice) and take multiple sips. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/D1.jpg\" alt=\"Person holding phone\" width=\"300\">\n",
    "<img src=\"Images/D2.jpg\" alt=\"Person holding phone\" width=\"300\">\n",
    "<img src=\"Images/D3.jpg\" alt=\"Person holding phone\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Here’s a revised version with improved grammar and flow:\n",
    "\n",
    "To achieve the best model performance, a large amount of data is required. However, creating a prototype model using a smaller dataset is still feasible.\n",
    "\n",
    "In our case, three subjects perform each activity 10 times, resulting in 30 datasets per activity across three subjects. This brings the total dataset size to 90 measurements.\n",
    "\n",
    "The recorded data is compressed into various features, including minimum, maximum, mean, and standard deviation values, resulting in 38 features per measurement. This gives the dataset a final shape of 90x38.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "Two significant problems were encountered during data collection:\n",
    "1) Interpreting and compressing the raw data into useful training data.\n",
    "2) Aligning different naming conventions, as various phone brands produced results with differing labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zP8ihRoAd_oY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data saved to datasets/testSet\\data_total.csv\n",
      "(90, 38)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the root folder containing the action folders\n",
    "root_folder = \"datasets/testSet\"\n",
    "\n",
    "# Create an empty list to hold the summarized data\n",
    "summary_data = []\n",
    "\n",
    "# Iterate through all folders in the root folder\n",
    "for folder_name in os.listdir(root_folder):\n",
    "    folder_path = os.path.join(root_folder, folder_name)\n",
    "\n",
    "    # Check if the folder_path is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Extract person and action from the folder name\n",
    "        parts = folder_name.split(\"-\")\n",
    "        if len(parts) >= 2:\n",
    "            person = parts[0]\n",
    "            action_with_index = parts[1].rsplit(\" \", 1)[0]\n",
    "            action = ''.join([i for i in action_with_index if not i.isdigit()])\n",
    "\n",
    "            summary_row = {\n",
    "                'subject': person,\n",
    "                'Activity': action,\n",
    "                \n",
    "            }\n",
    "\n",
    "            # Iterate through all CSV files in the folder\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    try:\n",
    "                        # Read the CSV file\n",
    "                        data = pd.read_csv(file_path)\n",
    "\n",
    "                        # Skip empty files\n",
    "                        if data.empty:\n",
    "                            print(f\"Skipping empty file: {file_path}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Transfer names to insure device campatibility (Apple using different naming convension)\n",
    "                        if {\"X (m/s^2)\", \"Y (m/s^2)\", \"Z (m/s^2)\"}.issubset(data.columns) and file_name == \"Accelerometer.csv\":\n",
    "                            data.rename(columns={\n",
    "                                \"X (m/s^2)\": \"Acceleration x (m/s^2)\",\n",
    "                                \"Y (m/s^2)\": \"Acceleration y (m/s^2)\",\n",
    "                                \"Z (m/s^2)\": \"Acceleration z (m/s^2)\"\n",
    "                            }, inplace=True)\n",
    "                        elif {\"X (rad/s)\", \"Y (rad/s)\", \"Z (rad/s)\"}.issubset(data.columns):\n",
    "                            data.rename(columns={\n",
    "                                \"X (rad/s)\": 'Gyroscope x (rad/s)',\n",
    "                                \"Y (rad/s)\": 'Gyroscope y (rad/s)',\n",
    "                                \"Z (rad/s)\": 'Gyroscope z (rad/s)'\n",
    "                            }, inplace=True)\n",
    "                        elif {\"X (m/s^2)\",\"Y (m/s^2)\",\"Z (m/s^2)\"}.issubset(data.columns):\n",
    "                            data.rename(columns={\n",
    "                                \"X (m/s^2)\": 'Linear Acceleration x (m/s^2)',\n",
    "                                \"Y (m/s^2)\": 'Linear Acceleration y (m/s^2)',\n",
    "                                \"Z (m/s^2)\": 'Linear Acceleration z (m/s^2)'\n",
    "                            }, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "                        # Determine which type of data (Accelerometer, Gyroscope, or Linear Acceleration)\n",
    "                        if {'Acceleration x (m/s^2)', 'Acceleration y (m/s^2)', 'Acceleration z (m/s^2)'}.issubset(data.columns):\n",
    "                            data_type = \"Accelerometer\"\n",
    "                            columns = ['Acceleration x (m/s^2)', 'Acceleration y (m/s^2)', 'Acceleration z (m/s^2)']\n",
    "\n",
    "                        elif {'Gyroscope x (rad/s)', 'Gyroscope y (rad/s)', 'Gyroscope z (rad/s)'}.issubset(data.columns):\n",
    "                            data_type = \"Gyroscope\"\n",
    "                            columns = ['Gyroscope x (rad/s)', 'Gyroscope y (rad/s)', 'Gyroscope z (rad/s)']\n",
    "\n",
    "                        elif {'Linear Acceleration x (m/s^2)', 'Linear Acceleration y (m/s^2)', 'Linear Acceleration z (m/s^2)'}.issubset(data.columns):\n",
    "                            data_type = \"Linear Acceleration\"\n",
    "                            columns = ['Linear Acceleration x (m/s^2)', 'Linear Acceleration y (m/s^2)', 'Linear Acceleration z (m/s^2)']\n",
    "\n",
    "                        else:\n",
    "                            print(f\"File {file_path} does not contain recognized column names. Skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        # Calculate statistics for relevant columns\n",
    "                        mean_values = data[columns].mean()\n",
    "                        std_values = data[columns].std()\n",
    "                        min_values = data[columns].min()\n",
    "                        max_values = data[columns].max()\n",
    "\n",
    "                        # add to the summary row\n",
    "                        for col in columns:\n",
    "                            summary_row[f'{col}_mean'] = mean_values[col]\n",
    "                            summary_row[f'{col}_std'] = std_values[col]\n",
    "                            summary_row[f'{col}_min'] = min_values[col]\n",
    "                            summary_row[f'{col}_max'] = max_values[col]\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "            summary_data.append(summary_row)\n",
    "# Create a dataframe from the summary data\n",
    "if not summary_data:\n",
    "    print(\"No valid data found. Summary CSV will not be created.\")\n",
    "else:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    # Save the summarized data to a single CSV file\n",
    "    output_path = os.path.join(root_folder, \"data_total.csv\")\n",
    "    summary_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Summary data saved to {output_path}\")\n",
    "\n",
    "print(summary_df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (72, 38)\n",
      "Test set size: (18, 38)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'datasets/testSet/data_total.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Set your split ratio (e.g., 0.8 for 80% training, 20% testing)\n",
    "split_ratio = 0.8  # Change this value as desired (0 < split_ratio < 1)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_set, test_set = train_test_split(data, test_size=(1 - split_ratio), random_state=42)\n",
    "\n",
    "# Save the splits to new CSV files (optional)\n",
    "train_set.to_csv('datasets/train_set.csv', index=False)\n",
    "test_set.to_csv('datasets/test_set.csv', index=False)\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Train set size: {train_set.shape}\")\n",
    "print(f\"Test set size: {test_set.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phXQcbhjjc8E"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v8cn-1reCYH"
   },
   "source": [
    "# 3. Explore the data to gain insights.\n",
    "\n",
    "Explore the data in any possible way, visualize the results (if you have multiple plots of the same kind of data put them in one larger plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlhsbvzBeG4E"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZs7EkHfjhT9"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5kdsQwneWZ7"
   },
   "source": [
    "# 4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms\n",
    "\n",
    "prepare your data, is it normalized? are there outlier? Make a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0yqpHJUfUa5"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz04fD0bjls1"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY_GYjQSfWnF"
   },
   "source": [
    "# 5. Explore many different models and short-list the best ones.\n",
    "\n",
    "Explore / train and list the top 3 algorithms that score best on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAz7LbB2feBt"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwv3xz1wjows"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIL2-EUJffYV"
   },
   "source": [
    "# 6. Fine-tune your models and combine them into a great solution.\n",
    "\n",
    "can you get better performance within a model? e.g if you use a KNN classifier how does it behave if you change K (k=3 vs k=5 vs k=?). Which parameters are here to tune in the chosen models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bG5g80NfpMk"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvWBlVKeklAE"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvnEhvXwfrlm"
   },
   "source": [
    "# 7. Present your solution.\n",
    "\n",
    "Explain why you would choose for a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6NTnFVCfww-"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82zdBc53kmb4"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1hc2Ffofz79"
   },
   "source": [
    "# 8. Launch, monitor, and maintain your system.\n",
    "\n",
    "Can you Deployment the model?\n",
    "\n",
    "> NOTE: The app provides the option for remote access, so you are able to get live sensordata from the phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ1uQFlood1G"
   },
   "source": [
    "# 9. Additional Questions\n",
    "\n",
    "* Explain the chosen motions you chose to be classified. \n",
    "\n",
    "* Which of these motions is easier/harder to classify and why?\n",
    "\n",
    "* After your experience, which extra sensor data might help getting a better classifier and why?\n",
    "\n",
    "* Explain why you think that your chosen algorithm outperforms the rest? \n",
    "\n",
    "* While recording the same motions with the same sensor data, what do you think will help improving the performance of your models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi5EGcnIoiyL"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "comparing-classifiers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
